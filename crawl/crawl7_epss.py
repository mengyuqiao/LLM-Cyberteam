#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import json
import time
import random
import argparse
import re
from pathlib import Path
import logging
import cloudscraper
from datetime import datetime, timedelta
import pandas as pd
import signal
import sys

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GracefulInterruptHandler:
    """ä¼˜é›…åœ°å¤„ç†ä¸­æ–­ä¿¡å·"""
    def __init__(self):
        self.interrupted = False
        signal.signal(signal.SIGINT, self._handle_interrupt)
        signal.signal(signal.SIGTERM, self._handle_interrupt)
    
    def _handle_interrupt(self, signum, frame):
        self.interrupted = True
        logger.warning("\nâš ï¸ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")

interrupt_handler = GracefulInterruptHandler()

def try_parse_float(s):
    """å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œæ­£ç¡®å¤„ç†ç™¾åˆ†æ¯”æ ¼å¼"""
    try:
        if isinstance(s, str):
            # ç§»é™¤ç™¾åˆ†å·ã€åŠ å·ã€ç©ºæ ¼
            s = s.replace('%', '').replace('+', '').strip()
            # ä¿ç•™è´Ÿå·
            if s.startswith('-'):
                return -float(s[1:])
            else:
                return float(s)
        return float(s)
    except Exception:
        return None

def try_parse_date(date_str):
    """å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
    if not date_str:
        return None
    try:
        # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
        for fmt in ['%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y']:
            try:
                return datetime.strptime(date_str.strip(), fmt).date()
            except ValueError:
                continue
        return None
    except Exception:
        return None

def extract_epss_history_table(html_content: str, cve_id: str) -> list:
    """ä»HTMLå†…å®¹ä¸­æå–EPSSå†å²æ•°æ®è¡¨æ ¼"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾åŒ…å«EPSSå†å²æ•°æ®çš„è¡¨æ ¼
    tables = soup.find_all('table')
    result = []
    
    for table in tables:
        # æ£€æŸ¥è¡¨å¤´æ˜¯å¦åŒ…å«EPSSç›¸å…³å­—æ®µ
        ths = table.find_all('th')
        headers = [th.get_text(strip=True) for th in ths]
        
        # æŸ¥æ‰¾åŒ…å«Date, Old EPSS Score, New EPSS Score, Deltaç­‰å­—æ®µçš„è¡¨æ ¼
        if any('Date' in h for h in headers) and any('EPSS' in h for h in headers):
            logger.info(f"æ‰¾åˆ°EPSSå†å²è¡¨æ ¼ï¼Œè¡¨å¤´: {headers}")
            
            rows = table.find_all('tr')
            for i in range(1, len(rows)):  # è·³è¿‡è¡¨å¤´
                row = rows[i]
                cells = row.find_all('td')
                
                if len(cells) < 5:  # éœ€è¦5åˆ—ï¼š#, Date, Old EPSS, New EPSS, Delta
                    continue
                
                def celltxt(idx):
                    return cells[idx].get_text(strip=True) if idx < len(cells) else None
                
                # æ ¹æ®å®é™…è¡¨æ ¼ç»“æ„æ­£ç¡®æå–æ•°æ®
                # åˆ—0: # (åºå·)
                # åˆ—1: Date
                # åˆ—2: Old EPSS Score  
                # åˆ—3: New EPSS Score
                # åˆ—4: Delta (New - Old)
                
                row_number = celltxt(0)  # åºå·
                date_str = celltxt(1)    # æ—¥æœŸ
                old_score_str = celltxt(2)  # æ—§åˆ†æ•°
                new_score_str = celltxt(3)  # æ–°åˆ†æ•°  
                delta_str = celltxt(4)      # å˜åŒ–é‡
                
                # è§£ææ•°æ®
                date_parsed = try_parse_date(date_str)
                
                # æ­£ç¡®å¤„ç†ç™¾åˆ†æ¯”æ ¼å¼ (0.09% -> 0.0009)
                old_score = try_parse_float(old_score_str) / 100 if try_parse_float(old_score_str) is not None else None
                new_score = try_parse_float(new_score_str) / 100 if try_parse_float(new_score_str) is not None else None
                
                # Deltaå¤„ç†ï¼ˆå¯èƒ½åŒ…å«+/-ç¬¦å·ï¼‰
                delta_clean = delta_str.replace('+', '').replace('-', '') if delta_str else None
                delta = try_parse_float(delta_clean) / 100 if try_parse_float(delta_clean) is not None else None
                
                # ç¡®å®šdeltaçš„æ­£è´Ÿå·
                if delta is not None and delta_str:
                    if delta_str.startswith('-') or (old_score and new_score and new_score < old_score):
                        delta = -abs(delta)
                    else:
                        delta = abs(delta)
                
                # éªŒè¯æ•°æ®ä¸€è‡´æ€§
                if old_score is not None and new_score is not None and delta is not None:
                    calculated_delta = new_score - old_score
                    if abs(calculated_delta - delta) > 0.0001:  # å…è®¸å°çš„æµ®ç‚¹è¯¯å·®
                        logger.warning(f"Deltaä¸ä¸€è‡´: è®¡ç®—å€¼={calculated_delta:.6f}, è§£æå€¼={delta:.6f}")
                        delta = calculated_delta  # ä½¿ç”¨è®¡ç®—å€¼
                
                # è®¡ç®—ç™¾åˆ†æ¯”å˜åŒ–
                percentage_change = None
                if old_score is not None and old_score > 0 and delta is not None:
                    percentage_change = (delta / old_score) * 100
                
                item = {
                    "date": date_parsed.isoformat() if date_parsed else date_str,
                    "old_score": old_score,
                    "new_score": new_score,
                    "delta": delta,
                    "percentage_change": percentage_change,
                    "raw_old_score": old_score_str,
                    "raw_new_score": new_score_str,
                    "raw_delta": delta_str,
                    "row_number": row_number
                }
                
                result.append(item)
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    result.sort(key=lambda x: x['date'] if x['date'] and '20' in str(x['date']) else '1900-01-01', reverse=True)
    return result

class EPSSHistoryScraper:
    def __init__(self, delay_range: tuple = (3, 8), timeout: int = 30):
        self.base_url = "https://www.cvedetails.com"
        self.delay_range = delay_range
        self.timeout = timeout
        
        # ä½¿ç”¨ä¸CVSSçˆ¬è™«ç›¸åŒçš„sessionåˆå§‹åŒ–ç­–ç•¥
        try:
            self.session = cloudscraper.create_scraper()
            logger.info("ä½¿ç”¨cloudscraperåˆ›å»ºä¼šè¯")
        except Exception:
            self.session = requests.Session()
            logger.info("ä½¿ç”¨æ ‡å‡†requestsåˆ›å»ºä¼šè¯")
        
        # ä½¿ç”¨ç›¸åŒçš„User-Agentæ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        self.base_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'DNT': '1',
            'Cache-Control': 'max-age=0'
        }
        
        self.session.headers.update(self.base_headers)
        self.session_initialized = False

    def initialize_session(self) -> bool:
        """åˆå§‹åŒ–ä¼šè¯ - å¤ç”¨CVSSçˆ¬è™«çš„é€»è¾‘"""
        if self.session_initialized:
            return True
        try:
            user_agent = random.choice(self.user_agents)
            self.session.headers['User-Agent'] = user_agent
            self.session_initialized = True
            return True
        except Exception as e:
            logger.error(f"ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def get_safe_request(self, url: str, referer: str = None, max_retries: int = 3) -> requests.Response:
        """å®‰å…¨è¯·æ±‚æ–¹æ³• - å¤ç”¨CVSSçˆ¬è™«çš„åçˆ¬è™«ç­–ç•¥"""
        if not self.session_initialized:
            if not self.initialize_session():
                logger.error("æ— æ³•åˆå§‹åŒ–ä¼šè¯")
                return None
                
        for attempt in range(max_retries):
            try:
                if referer:
                    self.session.headers['Referer'] = referer
                
                # éšæœºå»¶è¿Ÿ
                delay = random.uniform(*self.delay_range)
                logger.info(f"ç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)
                
                # éšæœºæ›´æ¢User-Agent
                if random.random() < 0.3:
                    self.session.headers['User-Agent'] = random.choice(self.user_agents)
                
                logger.info(f"æ­£åœ¨è®¿é—®: {url} (å°è¯• {attempt + 1}/{max_retries})")
                response = self.session.get(url, timeout=self.timeout)
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 403:
                    wait_time = (attempt + 1) * 30
                    logger.warning(f"æ”¶åˆ°403é”™è¯¯ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    self.session_initialized = False
                    if not self.initialize_session():
                        logger.error("é‡æ–°åˆå§‹åŒ–ä¼šè¯å¤±è´¥")
                        continue
                elif response.status_code == 404:
                    logger.info(f"é¡µé¢ä¸å­˜åœ¨: {url}")
                    return None
                else:
                    logger.warning(f"HTTPçŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(10, 20))
                continue
                
        logger.error(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†: {url}")
        return None

    def get_epss_history(self, cve_id: str):
        """è·å–å•ä¸ªCVEçš„EPSSå†å²æ•°æ®"""
        # æ„å»ºEPSSå†å²é¡µé¢URL - æ ¹æ®å®é™…URLæ ¼å¼è°ƒæ•´
        epss_url = f"{self.base_url}/epss/{cve_id}/epss-score-history.html"
        
        response = self.get_safe_request(epss_url, referer=f"{self.base_url}/cve/{cve_id}/")
        
        if response:
            epss_history = extract_epss_history_table(response.text, cve_id)
            if epss_history and len(epss_history) > 0:
                logger.info(f"âœ“ {cve_id}: {len(epss_history)} æ¡EPSSå†å²è®°å½•")
                return epss_history
            else:
                # ä¿å­˜debugé¡µé¢
                debug_file = f"debug_epss_{cve_id}.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(response.text)
                logger.warning(f"æœªæ‰¾åˆ°EPSSå†å²æ•°æ®ï¼Œå·²ä¿å­˜debugæ–‡ä»¶: {debug_file}")
        
        logger.warning(f"EPSSå†å²è·å–å¤±è´¥: {cve_id}")
        return []

    def scrape_batch(self, cve_ids, output_file: Path, resume=True):
        """æ‰¹é‡çˆ¬å–EPSSå†å²æ•°æ®"""
        existing_data = {}
        
        # åŠ è½½ç°æœ‰æ•°æ®ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        if resume and output_file.exists():
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                logger.info(f"ğŸ“‚ åŠ è½½ç°æœ‰æ•°æ®: {len(existing_data)} ä¸ªCVE")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {e}")
        
        total = len(cve_ids)
        successful = 0
        failed = 0
        skipped = 0
        
        logger.info(f"ğŸ¯ å¼€å§‹æ‰¹é‡çˆ¬å–EPSSå†å²æ•°æ®...")
        logger.info(f"ğŸ“Š æ€»è®¡: {total} ä¸ªCVE")
        logger.info("\nğŸ’¡ æç¤º: æŒ‰ Ctrl+C å¯ä»¥å®‰å…¨ä¸­æ–­å¹¶ä¿å­˜è¿›åº¦\n")
        
        for i, cve_id in enumerate(cve_ids, 1):
            # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
            if interrupt_handler.interrupted:
                logger.warning("âš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
                self._save_data(existing_data, output_file)
                logger.info("âœ… è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä»¥éšæ—¶æ¢å¤")
                logger.info(f"ğŸ“Š å·²å¤„ç†: {i-1}/{total}")
                sys.exit(0)
            
            if cve_id in existing_data and existing_data[cve_id]:
                skipped += 1
                logger.info(f"[{i}/{total}] â­ï¸ è·³è¿‡å·²æœ‰æ•°æ®: {cve_id}")
                continue
            
            logger.info(f"[{i}/{total}] æ­£åœ¨å¤„ç†: {cve_id}")
            epss_history = self.get_epss_history(cve_id)
            
            # æ„å»ºæ•°æ®ç»“æ„
            existing_data[cve_id] = {
                "cve_id": cve_id,
                "epss_history": epss_history,
                "crawl_timestamp": datetime.now().isoformat(),
                "total_records": len(epss_history),
                "date_range": {
                    "earliest": epss_history[-1]['date'] if epss_history else None,
                    "latest": epss_history[0]['date'] if epss_history else None
                }
            }
            
            if epss_history:
                successful += 1
                logger.info(f"  âœ“ æˆåŠŸè·å– {len(epss_history)} æ¡è®°å½•")
                # æ˜¾ç¤ºæœ€è¿‘å‡ æ¡è®°å½•çš„ç®€è¦ä¿¡æ¯
                if len(epss_history) > 0:
                    latest = epss_history[0]
                    logger.info(f"  æœ€æ–°è®°å½•: {latest['date']} - EPSS: {latest['new_score']}")
            else:
                failed += 1
            
            # æ¯5ä¸ªCVEä¿å­˜ä¸€æ¬¡ï¼ˆé¿å…æ•°æ®ä¸¢å¤±ï¼‰
            if i % 5 == 0:
                self._save_data(existing_data, output_file)
                logger.info(f"ğŸ“ å·²ä¿å­˜è¿›åº¦ ({i}/{total})")
        
        # æœ€ç»ˆä¿å­˜
        self._save_data(existing_data, output_file)
        
        return {
            'total_cves': total,
            'successful': successful,
            'failed': failed,
            'skipped': skipped,
            'success_rate': successful / total * 100 if total > 0 else 0,
            'output_file': str(output_file)
        }

    def _save_data(self, data, output_file: Path):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_file = output_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False, sort_keys=True)
            # åŸå­æ€§æ›¿æ¢
            temp_file.replace(output_file)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")

    def generate_summary_report(self, data_file: Path):
        """ç”ŸæˆEPSSå†å²æ•°æ®çš„ç»Ÿè®¡æŠ¥å‘Š"""
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            total_cves = len(data)
            cves_with_data = sum(1 for cve_data in data.values() if cve_data.get('epss_history'))
            total_records = sum(len(cve_data.get('epss_history', [])) for cve_data in data.values())
            
            # åˆ†æEPSSåˆ†æ•°åˆ†å¸ƒ
            all_scores = []
            date_range = {'earliest': None, 'latest': None}
            
            for cve_data in data.values():
                for record in cve_data.get('epss_history', []):
                    if record.get('new_score') is not None:
                        all_scores.append(record['new_score'])
                    
                    if record.get('date'):
                        if not date_range['earliest'] or record['date'] < date_range['earliest']:
                            date_range['earliest'] = record['date']
                        if not date_range['latest'] or record['date'] > date_range['latest']:
                            date_range['latest'] = record['date']
            
            summary = {
                'collection_info': {
                    'total_cves': total_cves,
                    'cves_with_epss_data': cves_with_data,
                    'total_epss_records': total_records,
                    'coverage_rate': (cves_with_data / total_cves * 100) if total_cves > 0 else 0
                },
                'temporal_coverage': date_range,
                'epss_statistics': {
                    'total_scores': len(all_scores),
                    'mean_score': sum(all_scores) / len(all_scores) if all_scores else 0,
                    'min_score': min(all_scores) if all_scores else None,
                    'max_score': max(all_scores) if all_scores else None,
                    'scores_above_0.1': sum(1 for s in all_scores if s > 0.001) if all_scores else 0,
                    'scores_above_0.5': sum(1 for s in all_scores if s > 0.005) if all_scores else 0
                }
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return None

def generate_cve_range(start: str, end: str) -> list:
    """ç”ŸæˆCVEèŒƒå›´åˆ—è¡¨ï¼Œä¾‹å¦‚ CVE-2024-9894 åˆ° CVE-2024-9999"""
    try:
        # è§£æèµ·å§‹å’Œç»“æŸCVE
        start_parts = start.split('-')
        end_parts = end.split('-')
        
        if len(start_parts) != 3 or len(end_parts) != 3:
            raise ValueError("CVEæ ¼å¼é”™è¯¯")
        
        year = start_parts[1]
        start_num = int(start_parts[2])
        end_num = int(end_parts[2])
        
        cve_list = []
        for num in range(start_num, end_num + 1):
            cve_list.append(f"CVE-{year}-{num}")
        
        return cve_list
        
    except Exception as e:
        logger.error(f"ç”ŸæˆCVEèŒƒå›´å¤±è´¥: {e}")
        return []

def main():
    parser = argparse.ArgumentParser(description="EPSSå†å²æ•°æ®æ‰¹é‡çˆ¬è™«")
    parser.add_argument("--cve_ids", nargs='+', help="CVE IDåˆ—è¡¨")
    parser.add_argument("--cve_file", help="åŒ…å«CVE IDçš„æ–‡æœ¬æ–‡ä»¶")
    parser.add_argument("--cve_range", help="CVEèŒƒå›´ï¼Œæ ¼å¼: CVE-2024-9894:CVE-2024-9999")
    parser.add_argument("--output", default="./epss_history_data.json", help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--delay_min", type=float, default=3.0, help="æœ€å°è¯·æ±‚é—´éš”ç§’æ•°")
    parser.add_argument("--delay_max", type=float, default=8.0, help="æœ€å¤§è¯·æ±‚é—´éš”ç§’æ•°")
    parser.add_argument("--no_resume", action="store_true", help="ä¸ä»ç°æœ‰æ–‡ä»¶æ¢å¤")
    parser.add_argument("--generate_report", action="store_true", help="ç”Ÿæˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Š")
    args = parser.parse_args()

    # è·å–CVEåˆ—è¡¨
    cve_ids = []
    if args.cve_range:
        # è§£æèŒƒå›´æ ¼å¼
        if ':' in args.cve_range:
            start_cve, end_cve = args.cve_range.split(':')
            cve_ids = generate_cve_range(start_cve.strip(), end_cve.strip())
        else:
            logger.error("CVEèŒƒå›´æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º: CVE-2024-9894:CVE-2024-9999")
            return
    elif args.cve_file:
        with open(args.cve_file, 'r', encoding='utf-8') as f:
            cve_ids = [line.strip() for line in f if line.strip()]
    elif args.cve_ids:
        cve_ids = args.cve_ids
    else:
        logger.error("âŒ é”™è¯¯: å¿…é¡»æŒ‡å®š --cve_ids, --cve_file æˆ– --cve_range")
        return

    # æ ‡å‡†åŒ–CVE IDæ ¼å¼
    normalized_cve_ids = []
    for cve_id in cve_ids:
        cve_id = cve_id.strip().upper()
        if not cve_id.startswith('CVE-'):
            cve_id = f'CVE-{cve_id}'
        normalized_cve_ids.append(cve_id)

    output_path = Path(args.output)

    # å¦‚æœåªæ˜¯ç”ŸæˆæŠ¥å‘Š
    if args.generate_report:
        if output_path.exists():
            scraper = EPSSHistoryScraper()
            report = scraper.generate_summary_report(output_path)
            if report:
                print("\n" + "="*60)
                print("ğŸ“Š EPSSå†å²æ•°æ®ç»Ÿè®¡æŠ¥å‘Š")
                print("="*60)
                print(json.dumps(report, indent=2, ensure_ascii=False))
                print("="*60)
        else:
            logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {output_path}")
        return

    print("="*60)
    print("ğŸ•·ï¸ EPSSå†å²æ•°æ®æ‰¹é‡çˆ¬è™«")
    print("="*60)
    print(f"ğŸ“Š CVEæ•°é‡: {len(normalized_cve_ids)}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"â±ï¸ è¯·æ±‚é—´éš”: {args.delay_min}-{args.delay_max}ç§’")
    print(f"ğŸ”„ æ•°æ®æº: CVE Details EPSS History")
    if len(normalized_cve_ids) > 0:
        print(f"ğŸ“‹ CVEèŒƒå›´: {normalized_cve_ids[0]} ~ {normalized_cve_ids[-1]}")
    print("="*60)

    scraper = EPSSHistoryScraper((args.delay_min, args.delay_max))
    
    try:
        stats = scraper.scrape_batch(
            cve_ids=normalized_cve_ids,
            output_file=output_path,
            resume=not args.no_resume
        )
        
        print("\n" + "="*60)
        print("ğŸ“ˆ EPSSå†å²æ•°æ®çˆ¬å–å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
        print("="*60)
        print(f"ğŸ“Š æ€»CVEæ•°: {stats['total_cves']}")
        print(f"âœ… æˆåŠŸçˆ¬å–: {stats['successful']}")
        print(f"âŒ çˆ¬å–å¤±è´¥: {stats['failed']}")
        print(f"â­ï¸ è·³è¿‡å·²æœ‰: {stats['skipped']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {stats['output_file']}")
        print("="*60)
        
        # è‡ªåŠ¨ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        if stats['successful'] > 0:
            report = scraper.generate_summary_report(output_path)
            if report:
                print("\nğŸ“Š æ•°æ®ç»Ÿè®¡é¢„è§ˆ:")
                print(f"  æœ‰æ•ˆCVEæ•°: {report['collection_info']['cves_with_epss_data']}")
                print(f"  EPSSè®°å½•æ€»æ•°: {report['collection_info']['total_epss_records']}")
                print(f"  æ—¶é—´è·¨åº¦: {report['temporal_coverage']['earliest']} ~ {report['temporal_coverage']['latest']}")
                print(f"  å¹³å‡EPSSåˆ†æ•°: {report['epss_statistics']['mean_score']:.6f}")
        
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦")
        logger.info("ğŸ’¡ ä½¿ç”¨ç›¸åŒå‘½ä»¤å¯ä»¥ç»§ç»­ä¹‹å‰çš„çˆ¬å–è¿›åº¦")
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()