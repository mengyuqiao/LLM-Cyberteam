#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import json
import time
import random
import argparse
from pathlib import Path
import logging
import cloudscraper
from datetime import datetime
import signal
import sys

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GracefulInterruptHandler:
    def __init__(self):
        self.interrupted = False
        signal.signal(signal.SIGINT, self._handle_interrupt)
        signal.signal(signal.SIGTERM, self._handle_interrupt)
    def _handle_interrupt(self, signum, frame):
        self.interrupted = True
        logger.warning("\nâš ï¸ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")

interrupt_handler = GracefulInterruptHandler()

def try_parse_float(s):
    try:
        if isinstance(s, str):
            s = s.replace('%', '').replace('+', '').strip()
            if s.startswith('-'):
                return -float(s[1:])
            else:
                return float(s)
        return float(s)
    except Exception:
        return None

def try_parse_date(date_str):
    if not date_str:
        return None
    try:
        for fmt in ['%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y']:
            try:
                return datetime.strptime(date_str.strip(), fmt).date()
            except ValueError:
                continue
        return None
    except Exception:
        return None

def extract_epss_history_table(html_content: str, cve_id: str) -> list:
    soup = BeautifulSoup(html_content, 'html.parser')
    tables = soup.find_all('table')
    result = []
    for table in tables:
        ths = table.find_all('th')
        headers = [th.get_text(strip=True) for th in ths]
        if any('Date' in h for h in headers) and any('EPSS' in h for h in headers):
            logger.info(f"æ‰¾åˆ°EPSSå†å²è¡¨æ ¼ï¼Œè¡¨å¤´: {headers}")
            rows = table.find_all('tr')
            for i in range(1, len(rows)):
                row = rows[i]
                cells = row.find_all('td')
                if len(cells) < 5:
                    continue
                def celltxt(idx):
                    return cells[idx].get_text(strip=True) if idx < len(cells) else None
                row_number = celltxt(0)
                date_str = celltxt(1)
                old_score_str = celltxt(2)
                new_score_str = celltxt(3)
                delta_str = celltxt(4)
                date_parsed = try_parse_date(date_str)
                old_score = try_parse_float(old_score_str) / 100 if try_parse_float(old_score_str) is not None else None
                new_score = try_parse_float(new_score_str) / 100 if try_parse_float(new_score_str) is not None else None
                delta_clean = delta_str.replace('+', '').replace('-', '') if delta_str else None
                delta = try_parse_float(delta_clean) / 100 if try_parse_float(delta_clean) is not None else None
                if delta is not None and delta_str:
                    if delta_str.startswith('-') or (old_score and new_score and new_score < old_score):
                        delta = -abs(delta)
                    else:
                        delta = abs(delta)
                if old_score is not None and new_score is not None and delta is not None:
                    calculated_delta = new_score - old_score
                    if abs(calculated_delta - delta) > 0.0001:
                        logger.warning(f"Deltaä¸ä¸€è‡´: è®¡ç®—å€¼={calculated_delta:.6f}, è§£æå€¼={delta:.6f}")
                        delta = calculated_delta
                percentage_change = None
                if old_score is not None and old_score > 0 and delta is not None:
                    percentage_change = (delta / old_score) * 100
                item = {
                    "date": date_parsed.isoformat() if date_parsed else date_str,
                    "old_score": old_score,
                    "new_score": new_score,
                    "delta": delta,
                    "percentage_change": percentage_change,
                    "raw_old_score": old_score_str,
                    "raw_new_score": new_score_str,
                    "raw_delta": delta_str,
                    "row_number": row_number
                }
                result.append(item)
    result.sort(key=lambda x: x['date'] if x['date'] and '20' in str(x['date']) else '1900-01-01', reverse=True)
    return result

class EPSSHistoryScraper:
    def __init__(self, delay_range: tuple = (3, 8), timeout: int = 30):
        self.base_url = "https://www.cvedetails.com"
        self.delay_range = delay_range
        self.timeout = timeout
        try:
            self.session = cloudscraper.create_scraper()
            logger.info("ä½¿ç”¨cloudscraperåˆ›å»ºä¼šè¯")
        except Exception:
            self.session = requests.Session()
            logger.info("ä½¿ç”¨æ ‡å‡†requestsåˆ›å»ºä¼šè¯")
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        self.base_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'DNT': '1',
            'Cache-Control': 'max-age=0'
        }
        self.session.headers.update(self.base_headers)
        self.session_initialized = False

    def initialize_session(self) -> bool:
        if self.session_initialized:
            return True
        try:
            user_agent = random.choice(self.user_agents)
            self.session.headers['User-Agent'] = user_agent
            self.session_initialized = True
            return True
        except Exception as e:
            logger.error(f"ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def get_safe_request(self, url: str, referer: str = None, max_retries: int = 3) -> requests.Response:
        if not self.session_initialized:
            if not self.initialize_session():
                logger.error("æ— æ³•åˆå§‹åŒ–ä¼šè¯")
                return None
        for attempt in range(max_retries):
            try:
                if referer:
                    self.session.headers['Referer'] = referer
                delay = random.uniform(*self.delay_range)
                logger.info(f"ç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)
                if random.random() < 0.3:
                    self.session.headers['User-Agent'] = random.choice(self.user_agents)
                logger.info(f"æ­£åœ¨è®¿é—®: {url} (å°è¯• {attempt + 1}/{max_retries})")
                response = self.session.get(url, timeout=self.timeout)
                if response.status_code == 200:
                    return response
                elif response.status_code == 403:
                    wait_time = (attempt + 1) * 30
                    logger.warning(f"æ”¶åˆ°403é”™è¯¯ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    self.session_initialized = False
                    if not self.initialize_session():
                        logger.error("é‡æ–°åˆå§‹åŒ–ä¼šè¯å¤±è´¥")
                        continue
                elif response.status_code == 404:
                    logger.info(f"é¡µé¢ä¸å­˜åœ¨: {url}")
                    return None
                else:
                    logger.warning(f"HTTPçŠ¶æ€ç : {response.status_code}")
            except Exception as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(10, 20))
                continue
        logger.error(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†: {url}")
        return None

    def get_epss_history(self, cve_id: str):
        epss_url = f"{self.base_url}/epss/{cve_id}/epss-score-history.html"
        response = self.get_safe_request(epss_url, referer=f"{self.base_url}/cve/{cve_id}/")
        if response:
            epss_history = extract_epss_history_table(response.text, cve_id)
            if epss_history and len(epss_history) > 0:
                logger.info(f"âœ“ {cve_id}: {len(epss_history)} æ¡EPSSå†å²è®°å½•")
                return epss_history
            else:
                logger.warning(f"âŒ æœªæ‰¾åˆ° {cve_id} çš„EPSSå†å²æ•°æ®ã€‚")
        else:
            logger.warning(f"EPSSå†å²è·å–å¤±è´¥: {cve_id}")
        return []

    def scrape_batch(self, cve_ids, output_file: Path, resume=True):
        existing_data = {}
        if resume and output_file.exists():
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                logger.info(f"ğŸ“‚ åŠ è½½ç°æœ‰æ•°æ®: {len(existing_data)} ä¸ªCVE")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {e}")
        total = len(cve_ids)
        successful = 0
        failed = 0
        skipped = 0
        logger.info(f"ğŸ¯ å¼€å§‹æ‰¹é‡çˆ¬å–EPSSå†å²æ•°æ®...")
        logger.info(f"ğŸ“Š æ€»è®¡: {total} ä¸ªCVE")
        logger.info("\nğŸ’¡ æç¤º: æŒ‰ Ctrl+C å¯ä»¥å®‰å…¨ä¸­æ–­å¹¶ä¿å­˜è¿›åº¦\n")
        for i, cve_id in enumerate(cve_ids, 1):
            if interrupt_handler.interrupted:
                logger.warning("âš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
                self._save_data(existing_data, output_file)
                logger.info("âœ… è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä»¥éšæ—¶æ¢å¤")
                logger.info(f"ğŸ“Š å·²å¤„ç†: {i-1}/{total}")
                sys.exit(0)
            if cve_id in existing_data and existing_data[cve_id]:
                skipped += 1
                logger.info(f"[{i}/{total}] â­ï¸ è·³è¿‡å·²æœ‰æ•°æ®: {cve_id}")
                continue
            logger.info(f"[{i}/{total}] æ­£åœ¨å¤„ç†: {cve_id}")
            epss_history = self.get_epss_history(cve_id)
            if epss_history and len(epss_history) > 0:
                existing_data[cve_id] = {
                    "cve_id": cve_id,
                    "epss_history": epss_history,
                    "crawl_timestamp": datetime.now().isoformat(),
                    "total_records": len(epss_history),
                    "date_range": {
                        "earliest": epss_history[-1]['date'],
                        "latest": epss_history[0]['date']
                    }
                }
                successful += 1
                logger.info(f"  âœ“ æˆåŠŸè·å– {len(epss_history)} æ¡è®°å½•")
                if len(epss_history) > 0:
                    latest = epss_history[0]
                    logger.info(f"  æœ€æ–°è®°å½•: {latest['date']} - EPSS: {latest['new_score']}")
            else:
                failed += 1
                logger.warning(f"âŒ æœªæ‰¾åˆ° {cve_id} çš„EPSSå†å²æ•°æ®ï¼Œä¸ä¿å­˜è¯¥CVEã€‚")
            if i % 5 == 0:
                self._save_data(existing_data, output_file)
                logger.info(f"ğŸ“ å·²ä¿å­˜è¿›åº¦ ({i}/{total})")
        self._save_data(existing_data, output_file)
        return {
            'total_cves': total,
            'successful': successful,
            'failed': failed,
            'skipped': skipped,
            'success_rate': successful / total * 100 if total > 0 else 0,
            'output_file': str(output_file)
        }

    def _save_data(self, data, output_file: Path):
        try:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            temp_file = output_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False, sort_keys=True)
            temp_file.replace(output_file)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def generate_2025_cve_ids():
    cve_prefix = "CVE-2025-"
    max_cve_number = 99999
    cve_ids = [f"{cve_prefix}{str(i).zfill(4)}" for i in range(1, max_cve_number + 1)]
    return cve_ids

def main():
    parser = argparse.ArgumentParser(description="EPSSå†å²æ•°æ®æ‰¹é‡çˆ¬è™«")
    parser.add_argument("--crawl_2025", action="store_true", help="è‡ªåŠ¨çˆ¬å–2025å¹´æ‰€æœ‰CVE")
    parser.add_argument("--output", default=None, help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸å¡«åˆ™è‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³æ–‡ä»¶ï¼‰")
    parser.add_argument("--delay_min", type=float, default=3.0, help="æœ€å°è¯·æ±‚é—´éš”ç§’æ•°")
    parser.add_argument("--delay_max", type=float, default=8.0, help="æœ€å¤§è¯·æ±‚é—´éš”ç§’æ•°")
    parser.add_argument("--no_resume", action="store_true", help="ä¸ä»ç°æœ‰æ–‡ä»¶æ¢å¤")
    args = parser.parse_args()

    cve_ids = []
    if args.crawl_2025:
        cve_ids = generate_2025_cve_ids()
    else:
        logger.error("âŒ é”™è¯¯: ç›®å‰åªæ”¯æŒ --crawl_2025 è‡ªåŠ¨çˆ¬å–å…¨å¹´2025")
        return

    normalized_cve_ids = []
    for cve_id in cve_ids:
        cve_id = cve_id.strip().upper()
        if not cve_id.startswith('CVE-'):
            cve_id = f'CVE-{cve_id}'
        normalized_cve_ids.append(cve_id)
    total_to_crawl = len(normalized_cve_ids)

    if args.output:
        output_path = Path(args.output)
    else:
        nowstr = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = Path(f"./epss_history_2025_{nowstr}.json")

    print("="*60)
    print("ğŸ•·ï¸ EPSSå†å²æ•°æ®æ‰¹é‡çˆ¬è™«")
    print("="*60)
    print(f"ğŸ“Š CVEæ•°é‡: {total_to_crawl}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"â±ï¸ è¯·æ±‚é—´éš”: {args.delay_min}-{args.delay_max}ç§’")
    print(f"ğŸ”„ æ•°æ®æº: CVE Details EPSS History")
    print(f"ğŸ“‹ CVEèŒƒå›´: {normalized_cve_ids[0]} ~ {normalized_cve_ids[-1]}")
    print("="*60)

    scraper = EPSSHistoryScraper((args.delay_min, args.delay_max))
    try:
        stats = scraper.scrape_batch(
            cve_ids=normalized_cve_ids,
            output_file=output_path,
            resume=not args.no_resume
        )
        print("\n" + "="*60)
        print("ğŸ“ˆ EPSSå†å²æ•°æ®çˆ¬å–å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
        print("="*60)
        print(f"ğŸ“Š æ€»CVEæ•°: {stats['total_cves']}")
        print(f"âœ… æˆåŠŸçˆ¬å–: {stats['successful']}")
        print(f"âŒ çˆ¬å–å¤±è´¥: {stats['failed']}")
        print(f"â­ï¸ è·³è¿‡å·²æœ‰: {stats['skipped']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {stats['output_file']}")
        print("="*60)

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦")
        logger.info("ğŸ’¡ ä½¿ç”¨ç›¸åŒå‘½ä»¤å¯ä»¥ç»§ç»­ä¹‹å‰çš„çˆ¬å–è¿›åº¦")
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
